{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550bf3b2",
   "metadata": {},
   "source": [
    "# Q:Neural Network on Gas_Turbines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff9658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e01d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"E:\\\\Assisgnments\\\\NeuralNetwork\\\\gas_turbines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc72dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.6998</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>92.708</td>\n",
       "      <td>3.5236</td>\n",
       "      <td>19.683</td>\n",
       "      <td>1059.8</td>\n",
       "      <td>549.97</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.626</td>\n",
       "      <td>3.4467</td>\n",
       "      <td>82.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.7901</td>\n",
       "      <td>1011.6</td>\n",
       "      <td>91.983</td>\n",
       "      <td>3.5298</td>\n",
       "      <td>19.659</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.644</td>\n",
       "      <td>3.4874</td>\n",
       "      <td>82.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.7139</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>91.348</td>\n",
       "      <td>3.5088</td>\n",
       "      <td>19.673</td>\n",
       "      <td>1059.8</td>\n",
       "      <td>549.92</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.656</td>\n",
       "      <td>3.6043</td>\n",
       "      <td>83.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.7975</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>90.196</td>\n",
       "      <td>3.5141</td>\n",
       "      <td>19.634</td>\n",
       "      <td>1060.1</td>\n",
       "      <td>550.09</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.644</td>\n",
       "      <td>3.3943</td>\n",
       "      <td>82.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0820</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>88.597</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>23.406</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>550.21</td>\n",
       "      <td>131.70</td>\n",
       "      <td>11.679</td>\n",
       "      <td>1.9081</td>\n",
       "      <td>82.782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0  6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1  6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2  6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3  7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4  7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "5  7.6998  1010.7  92.708  3.5236  19.683  1059.8  549.97  114.72  10.626   \n",
       "6  7.7901  1011.6  91.983  3.5298  19.659  1060.0  549.87  114.71  10.644   \n",
       "7  7.7139  1012.7  91.348  3.5088  19.673  1059.8  549.92  114.71  10.656   \n",
       "8  7.7975  1013.8  90.196  3.5141  19.634  1060.1  550.09  114.72  10.644   \n",
       "9  8.0820  1015.0  88.597  4.0612  23.406  1083.0  550.21  131.70  11.679   \n",
       "\n",
       "       CO     NOX  \n",
       "0  3.1547  82.722  \n",
       "1  3.2363  82.776  \n",
       "2  3.2012  82.468  \n",
       "3  3.1923  82.670  \n",
       "4  3.2484  82.311  \n",
       "5  3.4467  82.409  \n",
       "6  3.4874  82.440  \n",
       "7  3.6043  83.010  \n",
       "8  3.3943  82.284  \n",
       "9  1.9081  82.782  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf04e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97bd489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>1083.798770</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>12.102353</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>16.527806</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>1.103196</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>1000.800000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>9.904400</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>1079.600000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>11.622000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>1088.700000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>12.025000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>12.578000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>1100.800000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>15.081000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TIT           TAT           TEY           CDP            CO  \\\n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  15039.000000   \n",
       "mean    1083.798770    545.396183    134.188464     12.102353      1.972499   \n",
       "std       16.527806      7.866803     15.829717      1.103196      2.222206   \n",
       "min     1000.800000    512.450000    100.170000      9.904400      0.000388   \n",
       "25%     1079.600000    542.170000    127.985000     11.622000      0.858055   \n",
       "50%     1088.700000    549.890000    133.780000     12.025000      1.390200   \n",
       "75%     1096.000000    550.060000    140.895000     12.578000      2.160400   \n",
       "max     1100.800000    550.610000    174.610000     15.081000     44.103000   \n",
       "\n",
       "                NOX  \n",
       "count  15039.000000  \n",
       "mean      68.190934  \n",
       "std       10.470586  \n",
       "min       27.765000  \n",
       "25%       61.303500  \n",
       "50%       66.601000  \n",
       "75%       73.935500  \n",
       "max      119.890000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612dc90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "15034    1\n",
       "15035    1\n",
       "15036    1\n",
       "15037    1\n",
       "15038    1\n",
       "Name: TEY, Length: 15039, dtype: category\n",
       "Categories (2, int64): [1 < 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.cut(data.TEY,bins=[0,135,200],labels=[1,2])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a375c",
   "metadata": {},
   "source": [
    "spliting x and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d7db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    " x= np.array(data.drop(columns='TEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0206735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_func(i):\n",
    "                 x=(i-i.min())/(i.max()-i.min())\n",
    "                 return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5186cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm=norm_func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69148fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00623094, 0.9156068 , 0.08793482, ..., 0.00963355, 0.00286547,\n",
       "        0.07514684],\n",
       "       [0.00616335, 0.91606102, 0.08822461, ..., 0.00962719, 0.0029396 ,\n",
       "        0.0751959 ],\n",
       "       [0.00626573, 0.91642439, 0.08715357, ..., 0.00962992, 0.00290772,\n",
       "        0.0749161 ],\n",
       "       ...,\n",
       "       [0.00659912, 0.91415331, 0.09038485, ..., 0.00952273, 0.00723366,\n",
       "        0.08258689],\n",
       "       [0.00636411, 0.91460753, 0.08994154, ..., 0.00956815, 0.00567679,\n",
       "        0.0846899 ],\n",
       "       [0.00629316, 0.9149709 , 0.08860161, ..., 0.00961357, 0.00452509,\n",
       "        0.08402766]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e371fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test =train_test_split(x_norm,y,test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c749890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89adbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=10, activation='linear'))\n",
    "model.add(Dense(4,activation='tanh'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4367ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93cada9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "843/843 [==============================] - 1s 672us/step - loss: -1.0376 - accuracy: 0.7051 - val_loss: -1.7467 - val_accuracy: 0.7003\n",
      "Epoch 2/120\n",
      "843/843 [==============================] - 0s 558us/step - loss: -2.2620 - accuracy: 0.7051 - val_loss: -2.8596 - val_accuracy: 0.7003\n",
      "Epoch 3/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -3.3640 - accuracy: 0.7051 - val_loss: -3.9873 - val_accuracy: 0.7003\n",
      "Epoch 4/120\n",
      "843/843 [==============================] - 1s 641us/step - loss: -4.4773 - accuracy: 0.7051 - val_loss: -5.1167 - val_accuracy: 0.7003\n",
      "Epoch 5/120\n",
      "843/843 [==============================] - 0s 563us/step - loss: -5.5886 - accuracy: 0.7051 - val_loss: -6.2475 - val_accuracy: 0.7003\n",
      "Epoch 6/120\n",
      "843/843 [==============================] - 0s 566us/step - loss: -6.7088 - accuracy: 0.7051 - val_loss: -7.3880 - val_accuracy: 0.7003\n",
      "Epoch 7/120\n",
      "843/843 [==============================] - 1s 610us/step - loss: -7.8257 - accuracy: 0.7051 - val_loss: -8.5216 - val_accuracy: 0.7003\n",
      "Epoch 8/120\n",
      "843/843 [==============================] - 0s 574us/step - loss: -8.9385 - accuracy: 0.7051 - val_loss: -9.6540 - val_accuracy: 0.7003\n",
      "Epoch 9/120\n",
      "843/843 [==============================] - 0s 562us/step - loss: -10.0504 - accuracy: 0.7051 - val_loss: -10.7835 - val_accuracy: 0.7003\n",
      "Epoch 10/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -11.1640 - accuracy: 0.7051 - val_loss: -11.9133 - val_accuracy: 0.7003\n",
      "Epoch 11/120\n",
      "843/843 [==============================] - 0s 584us/step - loss: -12.2722 - accuracy: 0.7051 - val_loss: -13.0408 - val_accuracy: 0.7003\n",
      "Epoch 12/120\n",
      "843/843 [==============================] - 1s 603us/step - loss: -13.3859 - accuracy: 0.7051 - val_loss: -14.1776 - val_accuracy: 0.7003\n",
      "Epoch 13/120\n",
      "843/843 [==============================] - 1s 653us/step - loss: -14.5041 - accuracy: 0.7051 - val_loss: -15.3151 - val_accuracy: 0.7003\n",
      "Epoch 14/120\n",
      "843/843 [==============================] - 0s 572us/step - loss: -15.6221 - accuracy: 0.7051 - val_loss: -16.4503 - val_accuracy: 0.7003\n",
      "Epoch 15/120\n",
      "843/843 [==============================] - 0s 572us/step - loss: -16.7377 - accuracy: 0.7051 - val_loss: -17.5814 - val_accuracy: 0.7003\n",
      "Epoch 16/120\n",
      "843/843 [==============================] - 0s 557us/step - loss: -17.8501 - accuracy: 0.7051 - val_loss: -18.7149 - val_accuracy: 0.7003\n",
      "Epoch 17/120\n",
      "843/843 [==============================] - 1s 604us/step - loss: -18.9638 - accuracy: 0.7051 - val_loss: -19.8468 - val_accuracy: 0.7003\n",
      "Epoch 18/120\n",
      "843/843 [==============================] - 0s 594us/step - loss: -20.0783 - accuracy: 0.7051 - val_loss: -20.9790 - val_accuracy: 0.7003\n",
      "Epoch 19/120\n",
      "843/843 [==============================] - 0s 560us/step - loss: -21.1918 - accuracy: 0.7051 - val_loss: -22.1107 - val_accuracy: 0.7003\n",
      "Epoch 20/120\n",
      "843/843 [==============================] - 1s 607us/step - loss: -22.3027 - accuracy: 0.7051 - val_loss: -23.2374 - val_accuracy: 0.7003\n",
      "Epoch 21/120\n",
      "843/843 [==============================] - 0s 574us/step - loss: -23.4150 - accuracy: 0.7051 - val_loss: -24.3681 - val_accuracy: 0.7003\n",
      "Epoch 22/120\n",
      "843/843 [==============================] - 0s 587us/step - loss: -24.5286 - accuracy: 0.7051 - val_loss: -25.5020 - val_accuracy: 0.7003\n",
      "Epoch 23/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -25.6431 - accuracy: 0.7051 - val_loss: -26.6343 - val_accuracy: 0.7003\n",
      "Epoch 24/120\n",
      "843/843 [==============================] - 0s 576us/step - loss: -26.7617 - accuracy: 0.7051 - val_loss: -27.7740 - val_accuracy: 0.7003\n",
      "Epoch 25/120\n",
      "843/843 [==============================] - 0s 577us/step - loss: -27.8777 - accuracy: 0.7051 - val_loss: -28.9094 - val_accuracy: 0.7003\n",
      "Epoch 26/120\n",
      "843/843 [==============================] - 1s 630us/step - loss: -28.9990 - accuracy: 0.7051 - val_loss: -30.0477 - val_accuracy: 0.7003\n",
      "Epoch 27/120\n",
      "843/843 [==============================] - 1s 629us/step - loss: -30.1162 - accuracy: 0.7051 - val_loss: -31.1796 - val_accuracy: 0.7003\n",
      "Epoch 28/120\n",
      "843/843 [==============================] - 0s 576us/step - loss: -31.2307 - accuracy: 0.7051 - val_loss: -32.3088 - val_accuracy: 0.7003\n",
      "Epoch 29/120\n",
      "843/843 [==============================] - 0s 588us/step - loss: -32.3400 - accuracy: 0.7051 - val_loss: -33.4395 - val_accuracy: 0.7003\n",
      "Epoch 30/120\n",
      "843/843 [==============================] - 1s 600us/step - loss: -33.4517 - accuracy: 0.7051 - val_loss: -34.5718 - val_accuracy: 0.7003\n",
      "Epoch 31/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -34.5697 - accuracy: 0.7051 - val_loss: -35.7101 - val_accuracy: 0.7003\n",
      "Epoch 32/120\n",
      "843/843 [==============================] - 0s 584us/step - loss: -35.6856 - accuracy: 0.7051 - val_loss: -36.8412 - val_accuracy: 0.7003\n",
      "Epoch 33/120\n",
      "843/843 [==============================] - 0s 570us/step - loss: -36.7997 - accuracy: 0.7051 - val_loss: -37.9735 - val_accuracy: 0.7003\n",
      "Epoch 34/120\n",
      "843/843 [==============================] - 0s 576us/step - loss: -37.9096 - accuracy: 0.7051 - val_loss: -39.0996 - val_accuracy: 0.7003\n",
      "Epoch 35/120\n",
      "843/843 [==============================] - 1s 599us/step - loss: -39.0214 - accuracy: 0.7051 - val_loss: -40.2318 - val_accuracy: 0.7003\n",
      "Epoch 36/120\n",
      "843/843 [==============================] - 0s 574us/step - loss: -40.1341 - accuracy: 0.7051 - val_loss: -41.3603 - val_accuracy: 0.7003\n",
      "Epoch 37/120\n",
      "843/843 [==============================] - 0s 591us/step - loss: -41.2446 - accuracy: 0.7051 - val_loss: -42.4920 - val_accuracy: 0.7003\n",
      "Epoch 38/120\n",
      "843/843 [==============================] - 0s 573us/step - loss: -42.3583 - accuracy: 0.7051 - val_loss: -43.6275 - val_accuracy: 0.7003\n",
      "Epoch 39/120\n",
      "843/843 [==============================] - 0s 582us/step - loss: -43.4788 - accuracy: 0.7051 - val_loss: -44.7685 - val_accuracy: 0.7003\n",
      "Epoch 40/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -44.5952 - accuracy: 0.7051 - val_loss: -45.8998 - val_accuracy: 0.7003\n",
      "Epoch 41/120\n",
      "843/843 [==============================] - 1s 599us/step - loss: -45.7140 - accuracy: 0.7051 - val_loss: -47.0378 - val_accuracy: 0.7003\n",
      "Epoch 42/120\n",
      "843/843 [==============================] - 0s 550us/step - loss: -46.8298 - accuracy: 0.7051 - val_loss: -48.1742 - val_accuracy: 0.7003\n",
      "Epoch 43/120\n",
      "843/843 [==============================] - 0s 572us/step - loss: -47.9465 - accuracy: 0.7051 - val_loss: -49.3053 - val_accuracy: 0.7003\n",
      "Epoch 44/120\n",
      "843/843 [==============================] - 1s 647us/step - loss: -49.0568 - accuracy: 0.7051 - val_loss: -50.4329 - val_accuracy: 0.7003\n",
      "Epoch 45/120\n",
      "843/843 [==============================] - 0s 560us/step - loss: -50.1676 - accuracy: 0.7051 - val_loss: -51.5634 - val_accuracy: 0.7003\n",
      "Epoch 46/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -51.2854 - accuracy: 0.7051 - val_loss: -52.6997 - val_accuracy: 0.7003\n",
      "Epoch 47/120\n",
      "843/843 [==============================] - 0s 555us/step - loss: -52.4036 - accuracy: 0.7051 - val_loss: -53.8340 - val_accuracy: 0.7003\n",
      "Epoch 48/120\n",
      "843/843 [==============================] - 1s 602us/step - loss: -53.5133 - accuracy: 0.7051 - val_loss: -54.9626 - val_accuracy: 0.7003\n",
      "Epoch 49/120\n",
      "843/843 [==============================] - 0s 562us/step - loss: -54.6233 - accuracy: 0.7051 - val_loss: -56.0914 - val_accuracy: 0.7003\n",
      "Epoch 50/120\n",
      "843/843 [==============================] - 1s 623us/step - loss: -55.7379 - accuracy: 0.7051 - val_loss: -57.2268 - val_accuracy: 0.7003\n",
      "Epoch 51/120\n",
      "843/843 [==============================] - 1s 637us/step - loss: -56.8551 - accuracy: 0.7051 - val_loss: -58.3645 - val_accuracy: 0.7003\n",
      "Epoch 52/120\n",
      "843/843 [==============================] - 0s 563us/step - loss: -57.9770 - accuracy: 0.7051 - val_loss: -59.5007 - val_accuracy: 0.7003\n",
      "Epoch 53/120\n",
      "843/843 [==============================] - 0s 574us/step - loss: -59.0913 - accuracy: 0.7051 - val_loss: -60.6321 - val_accuracy: 0.7003\n",
      "Epoch 54/120\n",
      "843/843 [==============================] - 0s 571us/step - loss: -60.2007 - accuracy: 0.7051 - val_loss: -61.7583 - val_accuracy: 0.7003\n",
      "Epoch 55/120\n",
      "843/843 [==============================] - 0s 547us/step - loss: -61.3118 - accuracy: 0.7051 - val_loss: -62.8885 - val_accuracy: 0.7003\n",
      "Epoch 56/120\n",
      "843/843 [==============================] - 0s 583us/step - loss: -62.4212 - accuracy: 0.7051 - val_loss: -64.0174 - val_accuracy: 0.7003\n",
      "Epoch 57/120\n",
      "843/843 [==============================] - 0s 569us/step - loss: -63.5345 - accuracy: 0.7051 - val_loss: -65.1513 - val_accuracy: 0.7003\n",
      "Epoch 58/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -64.6519 - accuracy: 0.7051 - val_loss: -66.2893 - val_accuracy: 0.7003\n",
      "Epoch 59/120\n",
      "843/843 [==============================] - 1s 650us/step - loss: -65.7696 - accuracy: 0.7051 - val_loss: -67.4252 - val_accuracy: 0.7003\n",
      "Epoch 60/120\n",
      "843/843 [==============================] - 0s 555us/step - loss: -66.8907 - accuracy: 0.7051 - val_loss: -68.5598 - val_accuracy: 0.7003\n",
      "Epoch 61/120\n",
      "843/843 [==============================] - 0s 572us/step - loss: -68.0030 - accuracy: 0.7051 - val_loss: -69.6936 - val_accuracy: 0.7003\n",
      "Epoch 62/120\n",
      "843/843 [==============================] - 0s 586us/step - loss: -69.1180 - accuracy: 0.7051 - val_loss: -70.8262 - val_accuracy: 0.7003\n",
      "Epoch 63/120\n",
      "843/843 [==============================] - 0s 572us/step - loss: -70.2333 - accuracy: 0.7051 - val_loss: -71.9615 - val_accuracy: 0.7003\n",
      "Epoch 64/120\n",
      "843/843 [==============================] - 0s 566us/step - loss: -71.3534 - accuracy: 0.7051 - val_loss: -73.0986 - val_accuracy: 0.7003\n",
      "Epoch 65/120\n",
      "843/843 [==============================] - 1s 593us/step - loss: -72.4714 - accuracy: 0.7051 - val_loss: -74.2329 - val_accuracy: 0.7003\n",
      "Epoch 66/120\n",
      "843/843 [==============================] - 0s 560us/step - loss: -73.5839 - accuracy: 0.7051 - val_loss: -75.3663 - val_accuracy: 0.7003\n",
      "Epoch 67/120\n",
      "843/843 [==============================] - 1s 653us/step - loss: -74.7003 - accuracy: 0.7051 - val_loss: -76.5015 - val_accuracy: 0.7003\n",
      "Epoch 68/120\n",
      "843/843 [==============================] - 0s 587us/step - loss: -75.8171 - accuracy: 0.7051 - val_loss: -77.6376 - val_accuracy: 0.7003\n",
      "Epoch 69/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -76.9325 - accuracy: 0.7051 - val_loss: -78.7707 - val_accuracy: 0.7003\n",
      "Epoch 70/120\n",
      "843/843 [==============================] - 1s 614us/step - loss: -78.0481 - accuracy: 0.7051 - val_loss: -79.9033 - val_accuracy: 0.7003\n",
      "Epoch 71/120\n",
      "843/843 [==============================] - 0s 591us/step - loss: -79.1649 - accuracy: 0.7051 - val_loss: -81.0421 - val_accuracy: 0.7003\n",
      "Epoch 72/120\n",
      "843/843 [==============================] - 0s 566us/step - loss: -80.2857 - accuracy: 0.7051 - val_loss: -82.1830 - val_accuracy: 0.7003\n",
      "Epoch 73/120\n",
      "843/843 [==============================] - 0s 574us/step - loss: -81.4118 - accuracy: 0.7051 - val_loss: -83.3249 - val_accuracy: 0.7003\n",
      "Epoch 74/120\n",
      "843/843 [==============================] - 0s 556us/step - loss: -82.5299 - accuracy: 0.7051 - val_loss: -84.4611 - val_accuracy: 0.7003\n",
      "Epoch 75/120\n",
      "843/843 [==============================] - 1s 649us/step - loss: -83.6434 - accuracy: 0.7051 - val_loss: -85.5910 - val_accuracy: 0.7003\n",
      "Epoch 76/120\n",
      "843/843 [==============================] - 1s 603us/step - loss: -84.7635 - accuracy: 0.7051 - val_loss: -86.7293 - val_accuracy: 0.7003\n",
      "Epoch 77/120\n",
      "843/843 [==============================] - 0s 551us/step - loss: -85.8800 - accuracy: 0.7051 - val_loss: -87.8616 - val_accuracy: 0.7003\n",
      "Epoch 78/120\n",
      "843/843 [==============================] - 0s 578us/step - loss: -86.9885 - accuracy: 0.7051 - val_loss: -88.9896 - val_accuracy: 0.7003\n",
      "Epoch 79/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -88.0959 - accuracy: 0.7051 - val_loss: -90.1133 - val_accuracy: 0.7003\n",
      "Epoch 80/120\n",
      "843/843 [==============================] - 0s 534us/step - loss: -89.2086 - accuracy: 0.7051 - val_loss: -91.2450 - val_accuracy: 0.7003\n",
      "Epoch 81/120\n",
      "843/843 [==============================] - 0s 552us/step - loss: -90.3180 - accuracy: 0.7051 - val_loss: -92.3741 - val_accuracy: 0.7003\n",
      "Epoch 82/120\n",
      "843/843 [==============================] - 0s 543us/step - loss: -91.4315 - accuracy: 0.7051 - val_loss: -93.5086 - val_accuracy: 0.7003\n",
      "Epoch 83/120\n",
      "843/843 [==============================] - 0s 570us/step - loss: -92.5463 - accuracy: 0.7051 - val_loss: -94.6420 - val_accuracy: 0.7003\n",
      "Epoch 84/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -93.6601 - accuracy: 0.7051 - val_loss: -95.7729 - val_accuracy: 0.7003\n",
      "Epoch 85/120\n",
      "843/843 [==============================] - 0s 543us/step - loss: -94.7770 - accuracy: 0.7051 - val_loss: -96.9099 - val_accuracy: 0.7003\n",
      "Epoch 86/120\n",
      "843/843 [==============================] - 0s 557us/step - loss: -95.8951 - accuracy: 0.7051 - val_loss: -98.0450 - val_accuracy: 0.7003\n",
      "Epoch 87/120\n",
      "843/843 [==============================] - 0s 552us/step - loss: -97.0128 - accuracy: 0.7051 - val_loss: -99.1835 - val_accuracy: 0.7003\n",
      "Epoch 88/120\n",
      "843/843 [==============================] - 0s 584us/step - loss: -98.1346 - accuracy: 0.7051 - val_loss: -100.3264 - val_accuracy: 0.7003\n",
      "Epoch 89/120\n",
      "843/843 [==============================] - 0s 553us/step - loss: -99.2534 - accuracy: 0.7051 - val_loss: -101.4573 - val_accuracy: 0.7003\n",
      "Epoch 90/120\n",
      "843/843 [==============================] - 0s 560us/step - loss: -100.3652 - accuracy: 0.7051 - val_loss: -102.5895 - val_accuracy: 0.7003\n",
      "Epoch 91/120\n",
      "843/843 [==============================] - 0s 591us/step - loss: -101.4815 - accuracy: 0.7051 - val_loss: -103.7260 - val_accuracy: 0.7003\n",
      "Epoch 92/120\n",
      "843/843 [==============================] - 0s 585us/step - loss: -102.5914 - accuracy: 0.7051 - val_loss: -104.8451 - val_accuracy: 0.7003\n",
      "Epoch 93/120\n",
      "843/843 [==============================] - 0s 563us/step - loss: -103.6987 - accuracy: 0.7051 - val_loss: -105.9759 - val_accuracy: 0.7003\n",
      "Epoch 94/120\n",
      "843/843 [==============================] - 0s 573us/step - loss: -104.8149 - accuracy: 0.7051 - val_loss: -107.1142 - val_accuracy: 0.7003\n",
      "Epoch 95/120\n",
      "843/843 [==============================] - 0s 561us/step - loss: -105.9301 - accuracy: 0.7051 - val_loss: -108.2467 - val_accuracy: 0.7003\n",
      "Epoch 96/120\n",
      "843/843 [==============================] - 0s 561us/step - loss: -107.0469 - accuracy: 0.7051 - val_loss: -109.3814 - val_accuracy: 0.7003\n",
      "Epoch 97/120\n",
      "843/843 [==============================] - 0s 566us/step - loss: -108.1663 - accuracy: 0.7051 - val_loss: -110.5225 - val_accuracy: 0.7003\n",
      "Epoch 98/120\n",
      "843/843 [==============================] - 0s 559us/step - loss: -109.2840 - accuracy: 0.7051 - val_loss: -111.6553 - val_accuracy: 0.7003\n",
      "Epoch 99/120\n",
      "843/843 [==============================] - 0s 558us/step - loss: -110.4041 - accuracy: 0.7051 - val_loss: -112.7959 - val_accuracy: 0.7003\n",
      "Epoch 100/120\n",
      "843/843 [==============================] - 0s 538us/step - loss: -111.5254 - accuracy: 0.7051 - val_loss: -113.9373 - val_accuracy: 0.7003\n",
      "Epoch 101/120\n",
      "843/843 [==============================] - 0s 569us/step - loss: -112.6493 - accuracy: 0.7051 - val_loss: -115.0803 - val_accuracy: 0.7003\n",
      "Epoch 102/120\n",
      "843/843 [==============================] - 0s 576us/step - loss: -113.7712 - accuracy: 0.7051 - val_loss: -116.2164 - val_accuracy: 0.7003\n",
      "Epoch 103/120\n",
      "843/843 [==============================] - 0s 569us/step - loss: -114.8826 - accuracy: 0.7051 - val_loss: -117.3467 - val_accuracy: 0.7003\n",
      "Epoch 104/120\n",
      "843/843 [==============================] - 1s 594us/step - loss: -116.0012 - accuracy: 0.7051 - val_loss: -118.4872 - val_accuracy: 0.7003\n",
      "Epoch 105/120\n",
      "843/843 [==============================] - 0s 570us/step - loss: -117.1261 - accuracy: 0.7051 - val_loss: -119.6301 - val_accuracy: 0.7003\n",
      "Epoch 106/120\n",
      "843/843 [==============================] - 0s 562us/step - loss: -118.2500 - accuracy: 0.7051 - val_loss: -120.7729 - val_accuracy: 0.7003\n",
      "Epoch 107/120\n",
      "843/843 [==============================] - 0s 575us/step - loss: -119.3654 - accuracy: 0.7051 - val_loss: -121.9027 - val_accuracy: 0.7003\n",
      "Epoch 108/120\n",
      "843/843 [==============================] - 0s 554us/step - loss: -120.4782 - accuracy: 0.7051 - val_loss: -123.0380 - val_accuracy: 0.7003\n",
      "Epoch 109/120\n",
      "843/843 [==============================] - 1s 647us/step - loss: -121.5989 - accuracy: 0.7051 - val_loss: -124.1748 - val_accuracy: 0.7003\n",
      "Epoch 110/120\n",
      "843/843 [==============================] - 0s 578us/step - loss: -122.7166 - accuracy: 0.7051 - val_loss: -125.3124 - val_accuracy: 0.7003\n",
      "Epoch 111/120\n",
      "843/843 [==============================] - 0s 584us/step - loss: -123.8346 - accuracy: 0.7051 - val_loss: -126.4457 - val_accuracy: 0.7003\n",
      "Epoch 112/120\n",
      "843/843 [==============================] - 1s 626us/step - loss: -124.9454 - accuracy: 0.7051 - val_loss: -127.5757 - val_accuracy: 0.7003\n",
      "Epoch 113/120\n",
      "843/843 [==============================] - 0s 582us/step - loss: -126.0628 - accuracy: 0.7051 - val_loss: -128.7117 - val_accuracy: 0.7003\n",
      "Epoch 114/120\n",
      "843/843 [==============================] - 0s 543us/step - loss: -127.1809 - accuracy: 0.7051 - val_loss: -129.8532 - val_accuracy: 0.7003\n",
      "Epoch 115/120\n",
      "843/843 [==============================] - 0s 579us/step - loss: -128.2998 - accuracy: 0.7051 - val_loss: -130.9829 - val_accuracy: 0.7003\n",
      "Epoch 116/120\n",
      "843/843 [==============================] - 0s 554us/step - loss: -129.4116 - accuracy: 0.7051 - val_loss: -132.1144 - val_accuracy: 0.7003\n",
      "Epoch 117/120\n",
      "843/843 [==============================] - 0s 591us/step - loss: -130.5292 - accuracy: 0.7051 - val_loss: -133.2530 - val_accuracy: 0.7003\n",
      "Epoch 118/120\n",
      "843/843 [==============================] - 0s 590us/step - loss: -131.6463 - accuracy: 0.7051 - val_loss: -134.3875 - val_accuracy: 0.7003\n",
      "Epoch 119/120\n",
      "843/843 [==============================] - 0s 557us/step - loss: -132.7618 - accuracy: 0.7051 - val_loss: -135.5229 - val_accuracy: 0.7003\n",
      "Epoch 120/120\n",
      "843/843 [==============================] - 0s 581us/step - loss: -133.8786 - accuracy: 0.7051 - val_loss: -136.6582 - val_accuracy: 0.7003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2568ec84ac0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train,y_train, validation_split=0.3, epochs=120, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3b596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/376 [==============================] - 0s 550us/step - loss: -135.1059 - accuracy: 0.7037\n",
      "accuracy: 70.37%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model train\n",
    "scores = model.evaluate(x_train,y_train)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "539e4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 600us/step - loss: -136.2693 - accuracy: 0.7011\n",
      "accuracy: 70.11%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model test\n",
    "scores = model.evaluate(x_test,y_test)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
